# speech synthesis paper list

- [SoundStream: An End-to-End Neural Audio Codec](https://arxiv.org/abs/2107.03312) (arxiv 2021) : https://github.com/google/lyra
- [AudioLM: a Language Modeling Approach to Audio Generation](https://arxiv.org/abs/2209.03143v2) (arxiv 2022)
- [High-Fidelity Audio Compression with Improved RVQGAN](https://openreview.net/forum?id=qjnl1QUnFA) (NeurIPS 2023) : https://github.com/descriptinc/descript-audio-codec
- [InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt](https://arxiv.org/abs/2301.13662v2) (arxiv 2023) : https://dongchaoyang.top/InstructTTS/
- [AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec](https://arxiv.org/abs/2305.16608) (ICASSP 2023) : https://bigpon.github.io/AudioDec_demo/
- [Soundstorm: Efficient parallel audio generation](https://openreview.net/forum?id=KknWbD5j95) (ICLR 2024)
- [High Fidelity Neural Audio Compression](https://openreview.net/forum?id=ivCd8z8zR2) (ICLR 2024) : https://github.com/facebookresearch/encodec | https://github.com/ZhikangNiu/encodec-pytorch
- [HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec](https://arxiv.org/abs/2305.02765v2) (arxiv 2023) : https://github.com/yangdongchao/AcademiCodec
- [FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec](https://arxiv.org/abs/2309.07405v2) (arxiv 2023) : https://github.com/modelscope/FunCodec
- [SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models](https://openreview.net/forum?id=AF9Q8Vip84) (ICLR 2024) : https://github.com/ZhangXInFD/SpeechTokenizer
- [SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound](https://arxiv.org/abs/2405.00233) (arxiv 2024) : https://github.com/haoheliu/SemantiCodec
- [HILCodec: High Fidelity and Lightweight Neural Audio Codec](https://arxiv.org/abs/2405.04752) (arxiv 2024) : https://github.com/aask1357/hilcodec
- [SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models](https://arxiv.org/abs/2406.02328v2) (arxiv 2024 - SQ-Codec) : https://simplespeech.github.io/simplespeechDemo/
- [Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder](https://arxiv.org/abs/2406.02940) (arxiv 2024)
- [UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding](https://arxiv.org/abs/2306.07547v6) (AAAI 2024) : https://github.com/X-LANCE/UniCATS-CTX-vec2wav
- [The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge](https://arxiv.org/abs/2404.06079v2) (Interspeech 2024)
- [BiVocoder: A Bidirectional Neural Vocoder Integrating Feature Extraction and Waveform Generation](https://arxiv.org/abs/2406.02162) (arxiv 2024) : https://redmist328.github.io/BiVcoder_demo
- [Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer](https://openreview.net/forum?id=TJNCnkDRkY) (ICLR 2024)
- [Spectral Codecs: Spectrogram-Based Audio Codecs for High Quality Speech Synthesis](https://arxiv.org/abs/2406.05298) (arxiv 2024, FSQ) : https://github.com/NVIDIA/NeMo
- [Finite Scalar Quantization: VQ-VAE Made Simple](https://openreview.net/forum?id=8ishA3LxN8) (FSQ, no codebook collapse) : https://github.com/google-research/google-research/tree/master/fsq
- [X-Codec: Unified Audio Tokenizer for Audio Language Model]() (2024) : https://github.com/zhenye234/xcodec
- [UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner](https://arxiv.org/abs/2406.10056) (arxiv 2024) : https://github.com/yangdongchao/LLM-Codec
- snac : https://github.com/hubertsiuzdak/snac
- [Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis](https://arxiv.org/abs/2306.00814) (arxiv 2023) : https://github.com/gemelo-ai/vocos
- [CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens](https://fun-audio-llm.github.io/pdf/CosyVoice_v1.pdf) (2024) : https://github.com/FunAudioLLM/CosyVoice
- [Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation](https://www.arxiv.org/abs/2406.07422) (InterSpeech 2024)
- [APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding](https://arxiv.org/abs/2402.10533) (arxiv 2024  - code comming soon) : https://github.com/YangAi520/APCodec
- [dMel: Speech Tokenization made Simple](https://arxiv.org/abs/2407.15835) (arxiv 2024 - code comming soon)
- [SuperCodec: A Neural Speech Codec with Selective Back-Projection Network](https://arxiv.org/abs/2407.20530) (arxiv 2024) : https://github.com/exercise-book-yq/Supercodec

## Zero-Shot TTS

- [Voiceflow: Efficient text-to-speech with rectified flow matching](https://arxiv.org/abs/2309.05027v2) (ICASSP 2024) : https://github.com/X-LANCE/VoiceFlow-TTS
- [Voicebox: Text-guided multilingual universal speech generation at scale](https://openreview.net/forum?id=gzCS252hCO) (NeurIPS 2023)
- [Matcha-tts: A fast tts architecture with conditional flow matching](https://arxiv.org/abs/2309.03199v2) (ICASSP 2024) : https://github.com/shivammehta25/Matcha-TTS
- [Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2301.02111v1) (arxiv 2023 - VALL-E) : https://github.com/microsoft/unilm
- [VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild](https://arxiv.org/abs/2403.16973v2) (arxiv 2024 - VoiceCraft) : https://github.com/jasonppy/VoiceCraft
- [NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](https://openreview.net/forum?id=Rc7dAwVL3v) (ICLR 2024)
- [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100v3) (arxiv 2024)
- [Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](https://openreview.net/forum?id=mvMI3N4AvD) (ICLR 2024)
- [HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling](https://arxiv.org/abs/2403.05989) (arxiv 2024)
- [ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control With Decoupled Codec](https://arxiv.org/abs/2406.01205) (arxiv 2024) : https://github.com/jishengpeng/ControlSpeech
- [XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model](https://arxiv.org/abs/2406.04904) (arxiv 2024) : https://edresson.github.io/XTTS/
- [VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2406.05370) (arxiv 2024)
- [Autoregressive Diffusion Transformer for Text-to-Speech Synthesis](https://www.arxiv.org/abs/2406.05551) (arxiv 2024) : https://ardit-tts.github.io/
- [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243) (arxiv 2023) : https://github.com/neonbjb/tortoise-tts
- [VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment](https://arxiv.org/abs/2406.07855) (arxiv 2024)
- [DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer](https://arxiv.org/abs/2406.11427) (arxiv 2024)
- [CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech](https://openreview.net/forum?id=ofzeypWosV) (ICLR 2024)
- [TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2406.15752) (arxiv 2024) : https://github.com/Ereboas/TacoLM
- [HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](https://arxiv.org/abs/2311.12454) : https://github.com/sh-lee-prml/HierSpeechpp
- [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](https://arxiv.org/abs/2406.18009) (arxiv 2024, similar to Seed-TTS)
- [Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization](https://arxiv.org/abs/2407.02243) (arxiv 2024, Human FeedBack)
- [CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens](https://fun-audio-llm.github.io/pdf/CosyVoice_v1.pdf) (2024) : https://github.com/FunAudioLLM/CosyVoice
- [FlashSpeech: Efficient Zero-Shot Speech Synthesis](https://arxiv.org/abs/2404.14700) (arxiv 2024 - code comming soon) : https://github.com/zhenye234/FlashSpeech
- [Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation](https://arxiv.org/abs/2408.00284) (arxiv 2024)

## Other

- [Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning](https://arxiv.org/abs/2405.18386v2) (arxiv 2024) : https://github.com/ldzhangyx/instruct-MusicGen
- [StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations](https://arxiv.org/abs/2404.14946) (ICASSP 2024) : https://github.com/X-LANCE/StoryTTS
- [CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations](https://arxiv.org/abs/2404.06690) (arxiv 2024 - multi-round dialogue speech generation)
- [SpeechAlign: Aligning Speech Generation to Human Preferences](https://arxiv.org/abs/2404.05600) (arxiv 2024 - Human Feedback) : https://github.com/0nutation/SpeechGPT
- [Enhancing Zero-shot Text-to-Speech Synthesis with Human Feedback](https://www.arxiv.org/abs/2406.00654) (arxiv 2024 - Human Feedback)
- [Seed-TTS: A Family of High-Quality Versatile Speech Generation Models](https://arxiv.org/abs/2406.02430) (arxiv 2024) : https://bytedancespeech.github.io/seedtts_tech_report/
- [WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark](https://arxiv.org/abs/2406.05763v2) (arxiv 2024) : https://huggingface.co/Wenetspeech4TTS
- [Natural language guidance of high-fidelity text-to-speech with synthetic annotations](https://arxiv.org/abs/2402.01912) (Prompt Control) : https://github.com/huggingface/parler-tts
- [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) (Prompt Control) : https://github.com/facebookresearch/audiocraft
- [Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision](https://arxiv.org/abs/2302.03540) : https://github.com/collabora/WhisperSpeech
- [High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model](https://arxiv.org/abs/2406.17310) (Transducer/End-to-End)
- [VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech](https://arxiv.org/abs/2401.14321) (Transducer)
- [Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction](https://arxiv.org/abs/2401.01498) (Transducer/End-to-End)
- [Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment](https://arxiv.org/abs/2406.17957v1) (Monotonic Alignment)
- EmotiVoice: https://github.com/netease-youdao/EmotiVoice
- [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) (arxiv 2024) : https://github.com/OpenMOSS/AnyGPT
- [Speech-Copilot: Leveraging Large Language Models for Speech Processing via Task Decomposition, Modularization, and Program Generation](https://arxiv.org/abs/2407.09886) (arxiv 2024) : ?? https://github.com/kuan2jiu99 (code coming soon)
- [Qwen2-Audio Technical Report](https://www.arxiv.org/abs/2407.10759) (arxiv 2024) : https://github.com/QwenLM/Qwen2-Audio (code coming soon)
- [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models](https://arxiv.org/abs/2311.07919) (arxiv 2023) : https://github.com/QwenLM/Qwen-Audio
- [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656) (arxiv 2024) : https://github.com/microsoft/SpeechT5/tree/main/WavLLM
- [SALMONN: Towards Generic Hearing Abilities for Large Language Models](https://arxiv.org/abs/2310.13289) (arxiv 2023) : https://github.com/bytedance/SALMONN
- [GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities](https://arxiv.org/abs/2406.11768) (arxiv 2024) : https://github.com/Sreyan88/GAMA
- [Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models](https://arxiv.org/abs/2407.13509) (arxiv 2024 - Spontaneous)
- [Stable Audio Open](https://arxiv.org/abs/2407.14358) (arxiv 2024) : https://huggingface.co/stabilityai/stable-audio-open-1.0
- [Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities](https://arxiv.org/abs/2402.01831) : https://github.com/NVIDIA/audio-flamingo
- EmoLLM: https://github.com/SmartFlowAI/EmoLLM/blob/main/generate_data/tutorial.md
- [Generative Expressive Conversational Speech Synthesis](https://arxiv.org/abs/2407.21491) (arxiv 2024 - comming soon) : https://github.com/walker-hyf/GPT-Talker
- [SpeechGPT2: End-to-End Human-Like Spoken Chatbot]() (comming soon) : https://github.com/0nutation/SpeechGPT
- [Language Model Can Listen While Speaking](https://arxiv.org/abs/2408.02622)
 
## GitHub

- ChatTTS: https://github.com/2noise/ChatTTS/tree/main
- OpenVoice: https://github.com/myshell-ai/OpenVoice
- GPT-SoVITS: https://github.com/RVC-Boss/GPT-SoVITS
- VoiceCraft: https://github.com/jasonppy/VoiceCraft
- YourTTS: https://github.com/Edresson/YourTTS
- Coqui: https://github.com/coqui-ai/TTS
- MARS5-TTS: https://github.com/Camb-ai/MARS5-TTS
- edge-tts: https://github.com/rany2/edge-tts
- metavoice-src: https://github.com/metavoiceio/metavoice-src
- StyleTTS2: https://github.com/yl4579/StyleTTS2
- open-tts-tracker: https://github.com/Vaibhavs10/open-tts-tracker
- Amphion: https://github.com/open-mmlab/Amphion
- CTranslate2: https://github.com/OpenNMT/CTranslate2
- CFM: https://github.com/atong01/conditional-flow-matching
- speech-trident: https://github.com/ga642381/speech-trident
- bark: https://github.com/suno-ai/bark
- LangGPT: https://github.com/langgptai/LangGPT (提示词工程)

## Nice Tool

- pytorch-OpCounter: https://github.com/Lyken17/pytorch-OpCounter
- rich: https://github.com/Textualize/rich
- argbind: https://github.com/pseeth/argbind/
- audiotools: https://github.com/descriptinc/audiotools
- hydra: https://github.com/facebookresearch/hydra
- joblib: https://github.com/joblib/joblib
- einops: https://github.com/arogozhnikov/einops
- safetensors: https://github.com/huggingface/safetensors
- OpenDiloco: https://github.com/PrimeIntellect-ai/OpenDiloco
- WeTextProcessing: https://github.com/wenet-e2e/WeTextProcessing
- zed: https://github.com/zed-industries/zed
- weekly: https://github.com/ljinkai/weekly
- tinygrad: https://github.com/tinygrad/tinygrad
- ffmpeg-normalize: https://github.com/slhck/ffmpeg-normalize
- kohya_ss: https://github.com/bmaltais/kohya_ss
- Lora-Training-in-Comfy: https://github.com/LarryJane491/Lora-Training-in-Comfy
- ComfyUI-Manager: https://github.com/ltdrdata/ComfyUI-Manager
- ComfyUI: https://github.com/comfyanonymous/ComfyUI
- CosyVoice+ComfyUI: https://github.com/AIFSH/CosyVoice-ComfyUI

## Reference

- [别慌！一文教你看懂GPT-4o背后的语音技术](https://zhuanlan.zhihu.com/p/698725358)
- [百花齐放的Audio Codec: 语音合成利器](https://zhuanlan.zhihu.com/p/696434090)
- [InterSpeech2024 Speech Processing Using Discrete Speech Units](https://interspeech2024.org/special-sessions-challenges/) : https://www.wavlab.org/activities/2024/Interspeech2024-Discrete-Speech-Unit-Challenge/ : https://huggingface.co/discrete-speech : [arxiv 2024](https://arxiv.org/abs/2406.07725)
- [Towards audio language modeling -- an overview](https://arxiv.org/abs/2402.13236)
- [Codec-SUPERB: An In-Depth Analysis of Sound Codec Models](https://arxiv.org/abs/2402.13071v2) : https://github.com/voidful/Codec-SUPERB
- [EMO-Codec: A Depth Look at Emotion Preservation Capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations](https://arxiv.org/abs/2407.15458)



